{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/opt/ml/dataset/train/train.csv')\n",
    "valid = pd.read_csv('/opt/ml/dataset/test/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>subject_entity</th>\n",
       "      <th>object_entity</th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ã€ˆSomethingã€‰ëŠ” ì¡°ì§€ í•´ë¦¬ìŠ¨ì´ ì“°ê³  ë¹„í‹€ì¦ˆê°€ 1969ë…„ ì•¨ë²” ã€ŠAbbey R...</td>\n",
       "      <td>{'word': 'ë¹„í‹€ì¦ˆ', 'start_idx': 24, 'end_idx': 26...</td>\n",
       "      <td>{'word': 'ì¡°ì§€ í•´ë¦¬ìŠ¨', 'start_idx': 13, 'end_idx':...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>í˜¸ë‚¨ì´ ê¸°ë°˜ì¸ ë°”ë¥¸ë¯¸ë˜ë‹¹Â·ëŒ€ì•ˆì‹ ë‹¹Â·ë¯¼ì£¼í‰í™”ë‹¹ì´ ìš°ì—¬ê³¡ì ˆ ëì— í•©ë‹¹í•´ ë¯¼ìƒë‹¹(ê°€ì¹­)ìœ¼...</td>\n",
       "      <td>{'word': 'ë¯¼ì£¼í‰í™”ë‹¹', 'start_idx': 19, 'end_idx': ...</td>\n",
       "      <td>{'word': 'ëŒ€ì•ˆì‹ ë‹¹', 'start_idx': 14, 'end_idx': 1...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Kë¦¬ê·¸2ì—ì„œ ì„±ì  1ìœ„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” ê´‘ì£¼FCëŠ” ì§€ë‚œ 26ì¼ í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹ìœ¼ë¡œë¶€í„°...</td>\n",
       "      <td>{'word': 'ê´‘ì£¼FC', 'start_idx': 21, 'end_idx': 2...</td>\n",
       "      <td>{'word': 'í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹', 'start_idx': 34, 'end_idx...</td>\n",
       "      <td>org:member_of</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>ê· ì¼ê°€ ìƒí™œìš©í’ˆì  (ì£¼)ì•„ì„±ë‹¤ì´ì†Œ(ëŒ€í‘œ ë°•ì •ë¶€)ëŠ” ì½”ë¡œë‚˜19 ë°”ì´ëŸ¬ìŠ¤ë¡œ ì–´ë ¤ì›€ì„ ê²ª...</td>\n",
       "      <td>{'word': 'ì•„ì„±ë‹¤ì´ì†Œ', 'start_idx': 13, 'end_idx': ...</td>\n",
       "      <td>{'word': 'ë°•ì •ë¶€', 'start_idx': 22, 'end_idx': 24...</td>\n",
       "      <td>org:top_members/employees</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1967ë…„ í”„ë¡œ ì•¼êµ¬ ë“œë˜í”„íŠ¸ 1ìˆœìœ„ë¡œ ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ì—ê²Œ ì…ë‹¨í•˜ë©´ì„œ ë“±ë²ˆí˜¸ëŠ” 8...</td>\n",
       "      <td>{'word': 'ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ', 'start_idx': 22, 'end_id...</td>\n",
       "      <td>{'word': '1967', 'start_idx': 0, 'end_idx': 3,...</td>\n",
       "      <td>no_relation</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7760</th>\n",
       "      <td>7760</td>\n",
       "      <td>ì½”ë¡œë‚˜19 ë°©ì—­ ì¡°ì¹˜ì˜ ì¼í™˜ìœ¼ë¡œ êµ­ë¯¼ì˜ ì›€ì§ì„ì„ í†µì œí•˜ë ¤ëŠ” ì •ë¶€ì˜ ì‹œë„ë¥¼ ì´íƒˆë¦¬ì•„ ...</td>\n",
       "      <td>{'word': 'ì •ë¶€', 'start_idx': 33, 'end_idx': 34,...</td>\n",
       "      <td>{'word': 'ì´íƒˆë¦¬ì•„', 'start_idx': 41, 'end_idx': 4...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7761</th>\n",
       "      <td>7761</td>\n",
       "      <td>ì„  ì—°êµ¬ì›ì€ â€œìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì¹˜ë£Œì œì¸ ì¼€ì´ìº¡ì´ 92ì–µì› íŒë§¤ë˜ë©´ì„œ 2019ë…„ ì—°ê°„ 3...</td>\n",
       "      <td>{'word': 'ì¢…ê·¼ë‹¹', 'start_idx': 133, 'end_idx': 1...</td>\n",
       "      <td>{'word': 'ì „ë…„', 'start_idx': 143, 'end_idx': 14...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7762</th>\n",
       "      <td>7762</td>\n",
       "      <td>í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬(ì‚¬ì¥ ì¡°ì„±ì™„)ëŠ” 8ì›” 1ì¼ë¶€ë¡œ, 3ê¸‰ ê°„ë¶€ì§ì›ì— ëŒ€í•œ ìŠ¹ì§„Â·ì´ë™ ì¸...</td>\n",
       "      <td>{'word': 'í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬', 'start_idx': 0, 'end_idx'...</td>\n",
       "      <td>{'word': 'ì¡°ì„±ì™„', 'start_idx': 12, 'end_idx': 14...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>7763</td>\n",
       "      <td>1987ë…„ B. ìŠˆë‚˜ì´ë”(B. Schneider)ì— ì˜í•´ ë§Œë“¤ì–´ì¡Œë‹¤.</td>\n",
       "      <td>{'word': 'B. ìŠˆë‚˜ì´ë”', 'start_idx': 6, 'end_idx':...</td>\n",
       "      <td>{'word': '1987ë…„', 'start_idx': 0, 'end_idx': 4...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikipedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7764</th>\n",
       "      <td>7764</td>\n",
       "      <td>ì´ìŠ¹ì˜¥ ê°•ì§„êµ°ìˆ˜ëŠ” 2ì¼ NHë†í˜‘ì€í–‰ ê°•ì§„êµ°ì§€ë¶€(ì§€ë¶€ì¥ ê°•ëŒ€í˜•)ë¥¼ ë°©ë¬¸í•´ ë¶€í’ˆÂ·ì†Œì¬Â·...</td>\n",
       "      <td>{'word': 'ì´ìŠ¹ì˜¥', 'start_idx': 0, 'end_idx': 2, ...</td>\n",
       "      <td>{'word': '2ì¼', 'start_idx': 10, 'end_idx': 11,...</td>\n",
       "      <td>100</td>\n",
       "      <td>wikitree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40235 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           sentence  \\\n",
       "0        0  ã€ˆSomethingã€‰ëŠ” ì¡°ì§€ í•´ë¦¬ìŠ¨ì´ ì“°ê³  ë¹„í‹€ì¦ˆê°€ 1969ë…„ ì•¨ë²” ã€ŠAbbey R...   \n",
       "1        1  í˜¸ë‚¨ì´ ê¸°ë°˜ì¸ ë°”ë¥¸ë¯¸ë˜ë‹¹Â·ëŒ€ì•ˆì‹ ë‹¹Â·ë¯¼ì£¼í‰í™”ë‹¹ì´ ìš°ì—¬ê³¡ì ˆ ëì— í•©ë‹¹í•´ ë¯¼ìƒë‹¹(ê°€ì¹­)ìœ¼...   \n",
       "2        2  Kë¦¬ê·¸2ì—ì„œ ì„±ì  1ìœ„ë¥¼ ë‹¬ë¦¬ê³  ìˆëŠ” ê´‘ì£¼FCëŠ” ì§€ë‚œ 26ì¼ í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹ìœ¼ë¡œë¶€í„°...   \n",
       "3        3  ê· ì¼ê°€ ìƒí™œìš©í’ˆì  (ì£¼)ì•„ì„±ë‹¤ì´ì†Œ(ëŒ€í‘œ ë°•ì •ë¶€)ëŠ” ì½”ë¡œë‚˜19 ë°”ì´ëŸ¬ìŠ¤ë¡œ ì–´ë ¤ì›€ì„ ê²ª...   \n",
       "4        4  1967ë…„ í”„ë¡œ ì•¼êµ¬ ë“œë˜í”„íŠ¸ 1ìˆœìœ„ë¡œ ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ì—ê²Œ ì…ë‹¨í•˜ë©´ì„œ ë“±ë²ˆí˜¸ëŠ” 8...   \n",
       "...    ...                                                ...   \n",
       "7760  7760  ì½”ë¡œë‚˜19 ë°©ì—­ ì¡°ì¹˜ì˜ ì¼í™˜ìœ¼ë¡œ êµ­ë¯¼ì˜ ì›€ì§ì„ì„ í†µì œí•˜ë ¤ëŠ” ì •ë¶€ì˜ ì‹œë„ë¥¼ ì´íƒˆë¦¬ì•„ ...   \n",
       "7761  7761  ì„  ì—°êµ¬ì›ì€ â€œìœ„ì‹ë„ì—­ë¥˜ì§ˆí™˜ì¹˜ë£Œì œì¸ ì¼€ì´ìº¡ì´ 92ì–µì› íŒë§¤ë˜ë©´ì„œ 2019ë…„ ì—°ê°„ 3...   \n",
       "7762  7762  í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬(ì‚¬ì¥ ì¡°ì„±ì™„)ëŠ” 8ì›” 1ì¼ë¶€ë¡œ, 3ê¸‰ ê°„ë¶€ì§ì›ì— ëŒ€í•œ ìŠ¹ì§„Â·ì´ë™ ì¸...   \n",
       "7763  7763             1987ë…„ B. ìŠˆë‚˜ì´ë”(B. Schneider)ì— ì˜í•´ ë§Œë“¤ì–´ì¡Œë‹¤.   \n",
       "7764  7764  ì´ìŠ¹ì˜¥ ê°•ì§„êµ°ìˆ˜ëŠ” 2ì¼ NHë†í˜‘ì€í–‰ ê°•ì§„êµ°ì§€ë¶€(ì§€ë¶€ì¥ ê°•ëŒ€í˜•)ë¥¼ ë°©ë¬¸í•´ ë¶€í’ˆÂ·ì†Œì¬Â·...   \n",
       "\n",
       "                                         subject_entity  \\\n",
       "0     {'word': 'ë¹„í‹€ì¦ˆ', 'start_idx': 24, 'end_idx': 26...   \n",
       "1     {'word': 'ë¯¼ì£¼í‰í™”ë‹¹', 'start_idx': 19, 'end_idx': ...   \n",
       "2     {'word': 'ê´‘ì£¼FC', 'start_idx': 21, 'end_idx': 2...   \n",
       "3     {'word': 'ì•„ì„±ë‹¤ì´ì†Œ', 'start_idx': 13, 'end_idx': ...   \n",
       "4     {'word': 'ìš”ë¯¸ìš°ë¦¬ ìì´ì–¸ì¸ ', 'start_idx': 22, 'end_id...   \n",
       "...                                                 ...   \n",
       "7760  {'word': 'ì •ë¶€', 'start_idx': 33, 'end_idx': 34,...   \n",
       "7761  {'word': 'ì¢…ê·¼ë‹¹', 'start_idx': 133, 'end_idx': 1...   \n",
       "7762  {'word': 'í•œêµ­ì „ê¸°ì•ˆì „ê³µì‚¬', 'start_idx': 0, 'end_idx'...   \n",
       "7763  {'word': 'B. ìŠˆë‚˜ì´ë”', 'start_idx': 6, 'end_idx':...   \n",
       "7764  {'word': 'ì´ìŠ¹ì˜¥', 'start_idx': 0, 'end_idx': 2, ...   \n",
       "\n",
       "                                          object_entity  \\\n",
       "0     {'word': 'ì¡°ì§€ í•´ë¦¬ìŠ¨', 'start_idx': 13, 'end_idx':...   \n",
       "1     {'word': 'ëŒ€ì•ˆì‹ ë‹¹', 'start_idx': 14, 'end_idx': 1...   \n",
       "2     {'word': 'í•œêµ­í”„ë¡œì¶•êµ¬ì—°ë§¹', 'start_idx': 34, 'end_idx...   \n",
       "3     {'word': 'ë°•ì •ë¶€', 'start_idx': 22, 'end_idx': 24...   \n",
       "4     {'word': '1967', 'start_idx': 0, 'end_idx': 3,...   \n",
       "...                                                 ...   \n",
       "7760  {'word': 'ì´íƒˆë¦¬ì•„', 'start_idx': 41, 'end_idx': 4...   \n",
       "7761  {'word': 'ì „ë…„', 'start_idx': 143, 'end_idx': 14...   \n",
       "7762  {'word': 'ì¡°ì„±ì™„', 'start_idx': 12, 'end_idx': 14...   \n",
       "7763  {'word': '1987ë…„', 'start_idx': 0, 'end_idx': 4...   \n",
       "7764  {'word': '2ì¼', 'start_idx': 10, 'end_idx': 11,...   \n",
       "\n",
       "                          label     source  \n",
       "0                   no_relation  wikipedia  \n",
       "1                   no_relation   wikitree  \n",
       "2                 org:member_of   wikitree  \n",
       "3     org:top_members/employees   wikitree  \n",
       "4                   no_relation  wikipedia  \n",
       "...                         ...        ...  \n",
       "7760                        100   wikitree  \n",
       "7761                        100   wikitree  \n",
       "7762                        100   wikitree  \n",
       "7763                        100  wikipedia  \n",
       "7764                        100   wikitree  \n",
       "\n",
       "[40235 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat([train, valid])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentence'].to_csv('train_val.txt',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['sentence'].to_csv('train.txt', index=False)\n",
    "valid['sentence'].to_csv('valid.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, AutoConfig, DataCollatorWithPadding, DataCollatorForLanguageModeling,EarlyStoppingCallback\n",
    "import torch\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/data/datasets/language_modeling.py:120: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ğŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mddobokki\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.4 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">mlm_roberta</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/chungye-mountain-sherpa/klue\" target=\"_blank\">https://wandb.ai/chungye-mountain-sherpa/klue</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/chungye-mountain-sherpa/klue/runs/2j7d7kkq\" target=\"_blank\">https://wandb.ai/chungye-mountain-sherpa/klue/runs/2j7d7kkq</a><br/>\n",
       "                Run data is saved locally in <code>/opt/ml/klue-level2-nlp-08/test_entitiy_masking/wandb/run-20211005_120119-2j7d7kkq</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 32471\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 128\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 3795\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "EarlyStoppingCallback requires load_best_model_at_end = True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-af4b88655140>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./kluebert-retrained\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1226\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1228\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1230\u001b[0m         \u001b[0;31m# Skip the first epochs_trained epochs to get the random state of the dataloader at the right point.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    388\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_best_model_at_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"EarlyStoppingCallback requires load_best_model_at_end = True\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         assert (\n\u001b[1;32m    543\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_for_best_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: EarlyStoppingCallback requires load_best_model_at_end = True"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('klue/roberta-large')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForMaskedLM.from_pretrained('klue/roberta-large')\n",
    "model.to(device)\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='train.txt',\n",
    "    block_size=512\n",
    ")\n",
    "\n",
    "valid_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path='valid.txt',\n",
    "    block_size=512\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(    # [MASK] ë¥¼ ì”Œìš°ëŠ” ê²ƒì€ ì €í¬ê°€ êµ¬í˜„í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤! :-)\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "        project=\"klue\",\n",
    "        entity=\"chungye-mountain-sherpa\",\n",
    "        name='mlm_roberta',\n",
    "        group='mlm',\n",
    "    )\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./kluebert-retrained\",\n",
    "    overwrite_output_dir=True,\n",
    "    learning_rate=5e-05,\n",
    "    num_train_epochs=15,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=2,\n",
    "    seed=42,\n",
    "    save_strategy='steps',\n",
    "    evaluation_strategy='steps',\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=100,\n",
    "    fp16=True,\n",
    "    fp16_opt_level='O1',\n",
    "    report_to=\"wandb\",\n",
    "    load_best_model_at_end = True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=valid_dataset,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=4)]\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./kluebert-retrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir=args.output_dir,\n",
    "# per_device_train_batch_size=hp_config['batch_size'],\n",
    "# per_device_eval_batch_size=hp_config['batch_size'],\n",
    "# gradient_accumulation_steps=hp_config['gradient_accumulation_steps'],\n",
    "# learning_rate=hp_config['learning_rate'],\n",
    "# weight_decay=hp_config['weight_decay'],\n",
    "# num_train_epochs=hp_config['epochs'],\n",
    "# logging_dir=args.logging_dir,\n",
    "# logging_steps=50,\n",
    "# save_total_limit=2,\n",
    "# evaluation_strategy=args.eval_strategy,\n",
    "# eval_steps=50,\n",
    "# save_steps=50,\n",
    "# # dataloader_num_workers=4,\n",
    "# load_best_model_at_end=True,\n",
    "# fp16=True,\n",
    "# fp16_opt_level='O1'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
